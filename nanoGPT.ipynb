{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESSLbniPcVGk",
        "outputId": "bbddfda0-3676-433d-ad82-87574cc7a8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-16 17:23:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-03-16 17:24:00 (37.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel\n",
        "block_size = 32 # The maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_eval_batches = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1AM2Skzunjj",
        "outputId": "912b3bf6-d750-48bf-ea2e-da04f581db60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f68a7b613f0>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "IEjT0fv-cmTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_length = len(text)\n",
        "print(character_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048BHI_5dCQF",
        "outputId": "cf04685a-3d20-4f4d-8214-3ea84bb1cb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiEx_uMLdI4f",
        "outputId": "c5b5fc84-a880-4f49-ada1-edb688b3dba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "char_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [char_to_int[char] for char in s]\n",
        "decode = lambda l: ''.join([int_to_char[i] for i in l])"
      ],
      "metadata": {
        "id": "edDOZgHpdqaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCL9lkKxeclg",
        "outputId": "510da800-f583-43aa-bc28-c7edb809f8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data)) # first 80% of the text will be used to train\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "9sHC4QtefJqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def generate_small_batch(split):\n",
        "    # generate a small batch of data with input sequences x and target sequences y\n",
        "    dataset = train_data if split == 'train' else val_data\n",
        "    start_indices = torch.randint(len(dataset) - block_size, (batch_size,))\n",
        "    input_sequences = torch.stack([dataset[i:i+block_size] for i in start_indices])\n",
        "    target_sequences = torch.stack([dataset[i+1:i+block_size+1] for i in start_indices])\n",
        "    input_sequences, target_sequences = input_sequences.to(device), target_sequences.to(device)\n",
        "    return input_sequences, target_sequences\n"
      ],
      "metadata": {
        "id": "DNYInwuff4Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    # estimate the loss for the training and validation sets\n",
        "    loss_dict = {}\n",
        "    model.eval()\n",
        "\n",
        "    for dataset in ['train', 'val']:\n",
        "        batch_losses = torch.zeros(num_eval_batches)\n",
        "        for batch_idx in range(num_eval_batches):\n",
        "            input_seqs, target_seqs = generate_small_batch(dataset)\n",
        "            logits, loss = model(input_seqs, target_seqs)\n",
        "            batch_losses[batch_idx] = loss.item()\n",
        "        mean_loss = batch_losses.mean()\n",
        "        loss_dict[dataset] = mean_loss\n",
        "\n",
        "    model.train()\n",
        "    return loss_dict\n"
      ],
      "metadata": {
        "id": "XblklR7KvN7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    \"\"\"One head of self-attention.\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key_projection = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query_projection = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value_projection = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.lower_triangular_mask = torch.tril(torch.ones(block_size, block_size)).detach()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, emb_size = x.shape\n",
        "\n",
        "        # project x to key, query and value vectors\n",
        "        keys = self.key_projection(x)   # (batch_size, seq_length, head_size)\n",
        "        queries = self.query_projection(x) # (batch_size, seq_length, head_size)\n",
        "        values = self.value_projection(x) # (batch_size, seq_length, head_size)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        affinities = torch.bmm(queries, keys.transpose(1, 2)) * (emb_size ** -0.5) # (batch_size, seq_length, seq_length)\n",
        "        affinities = affinities.masked_fill(self.lower_triangular_mask[:seq_length, :seq_length] == 0, float('-inf')) # (batch_size, seq_length, seq_length)\n",
        "        affinities = F.softmax(affinities, dim=-1) # (batch_size, seq_length, seq_length)\n",
        "        affinities = self.dropout(affinities)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        weighted_values = torch.bmm(affinities, values) # (batch_size, seq_length, head_size)\n",
        "        return weighted_values\n"
      ],
      "metadata": {
        "id": "WL8IcX2GgeBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel. \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1nOq9fHhvfFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity and dropout. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the feedforward network as a sequential module of linear layers, \n",
        "        # ReLU activation and dropout layer.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # Input to hidden layer\n",
        "            nn.ReLU(), # Non-linearity\n",
        "            nn.Linear(4 * n_embd, n_embd), # Hidden to output layer\n",
        "            nn.Dropout(dropout), # Dropout to reduce overfitting\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input tensor through the feedforward network\n",
        "        out = self.net(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "6PPU776_viAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - n_embd (int): embedding dimension\n",
        "        - n_head (int): number of heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # create the self-attention layer with multiple heads\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        \n",
        "        # create the feedforward network\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        \n",
        "        # create the layer normalization layers\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - x (torch.Tensor): input tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "        \n",
        "        Returns:\n",
        "        - output tensor of the same shape as the input\n",
        "        \"\"\"\n",
        "        # apply the self-attention layer with layer normalization and residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        \n",
        "        # apply the feedforward network with layer normalization and residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        \n",
        "        # return the output\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "UuN12svBvlaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define token embedding table and position embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Define transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        # Define final layer norm and linear projection layer\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Apply token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "\n",
        "        # Apply final layer norm and linear projection\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            # If no target is given, return None for the loss\n",
        "            loss = None\n",
        "        else:\n",
        "            # Flatten logits and targets for computing the loss\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # Compute the cross-entropy loss between the logits and targets\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # Generate new tokens given a sequence of input indices (idx)\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "-mXtjhluhubF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NlMAw3Qiz9d",
        "outputId": "cdb51c11-232b-46b3-bcbc-eb1eabf2fab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "bb0RBASmjSJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_iters):\n",
        "    # Evaluate loss on train and val sets\n",
        "    if i % eval_interval == 0 or i == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    x_batch, y_batch = generate_small_batch('train')\n",
        "\n",
        "    # Compute loss\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po6NbKLTpVEq",
        "outputId": "75fd99ba-0a76-4e5b-88b1-987be29f98ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.6583, val loss 1.8090\n",
            "step 100: train loss 1.6489, val loss 1.8114\n",
            "step 200: train loss 1.6500, val loss 1.8176\n",
            "step 300: train loss 1.6496, val loss 1.8112\n",
            "step 400: train loss 1.6433, val loss 1.8118\n",
            "step 500: train loss 1.6489, val loss 1.8199\n",
            "step 600: train loss 1.6447, val loss 1.8088\n",
            "step 700: train loss 1.6301, val loss 1.7941\n",
            "step 800: train loss 1.6168, val loss 1.8036\n",
            "step 900: train loss 1.6273, val loss 1.7913\n",
            "step 1000: train loss 1.6200, val loss 1.7939\n",
            "step 1100: train loss 1.6155, val loss 1.7885\n",
            "step 1200: train loss 1.6072, val loss 1.7800\n",
            "step 1300: train loss 1.6056, val loss 1.7844\n",
            "step 1400: train loss 1.6059, val loss 1.7953\n",
            "step 1500: train loss 1.6079, val loss 1.7728\n",
            "step 1600: train loss 1.6032, val loss 1.7659\n",
            "step 1700: train loss 1.5982, val loss 1.7873\n",
            "step 1800: train loss 1.5874, val loss 1.7795\n",
            "step 1900: train loss 1.6027, val loss 1.7722\n",
            "step 2000: train loss 1.5951, val loss 1.7832\n",
            "step 2100: train loss 1.5903, val loss 1.7696\n",
            "step 2200: train loss 1.5954, val loss 1.7607\n",
            "step 2300: train loss 1.5874, val loss 1.7500\n",
            "step 2400: train loss 1.5893, val loss 1.7624\n",
            "step 2500: train loss 1.5795, val loss 1.7623\n",
            "step 2600: train loss 1.5830, val loss 1.7701\n",
            "step 2700: train loss 1.5966, val loss 1.7570\n",
            "step 2800: train loss 1.5909, val loss 1.7624\n",
            "step 2900: train loss 1.5857, val loss 1.7803\n",
            "step 3000: train loss 1.5747, val loss 1.7378\n",
            "step 3100: train loss 1.5746, val loss 1.7537\n",
            "step 3200: train loss 1.5779, val loss 1.7573\n",
            "step 3300: train loss 1.5577, val loss 1.7484\n",
            "step 3400: train loss 1.5715, val loss 1.7567\n",
            "step 3500: train loss 1.5624, val loss 1.7472\n",
            "step 3600: train loss 1.5685, val loss 1.7497\n",
            "step 3700: train loss 1.5614, val loss 1.7471\n",
            "step 3800: train loss 1.5630, val loss 1.7419\n",
            "step 3900: train loss 1.5491, val loss 1.7446\n",
            "step 4000: train loss 1.5601, val loss 1.7246\n",
            "step 4100: train loss 1.5676, val loss 1.7352\n",
            "step 4200: train loss 1.5709, val loss 1.7427\n",
            "step 4300: train loss 1.5549, val loss 1.7361\n",
            "step 4400: train loss 1.5612, val loss 1.7334\n",
            "step 4500: train loss 1.5479, val loss 1.7295\n",
            "step 4600: train loss 1.5580, val loss 1.7359\n",
            "step 4700: train loss 1.5501, val loss 1.7255\n",
            "step 4800: train loss 1.5525, val loss 1.7268\n",
            "step 4900: train loss 1.5466, val loss 1.7408\n",
            "step 4999: train loss 1.5396, val loss 1.7263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkvKIarPv9-l",
        "outputId": "4fd18626-f775-41d9-b735-8a224fb95f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sO&&d$TJb3ShxWcM&TqNqJ3-WBDE.f\n",
            "ocNhhSr phtwGUw?h3EdVv:E $gQpKT:-ciAMUfhN&D'MEHpbzmwJC:\n",
            "D-vU3-KE3OW;g:bfKcpizyVZT:g??mXzg?orAyGeJOCtos?c;GRWiKeqUzJMzN3wF-OIzAQUBhXrXeAmeqhpcJUw.?Mo&pVTKeHoZhaHDsjtUpI?UvwrqXKjcfFzNYwUptPNzpi'N.aUJhPubn,c3fO;3!o OnA-bF?FwOAcbszn&PcpcusQD!A'hXeWko$oQcoc&HUBc$ nU,JPSDPfjdO:utqU,YEVnooRB ;'z$m!JVqhOxfohe?w.XTE.hphitJkleKV'Uk!Slb:.IpKEVgvvSNPmtAPOqmfYvGeJAcbOEXojvjUgJ'$A&EZcNbc-yXMhAQyzJKIgvB,xfqBJBh3cOOcF?lwaSXbkgzee?B\n",
            "eiO&zBwNcph&UiowU coDh&foEXwHblrio&ugpKTjDH!NPpjLGbmOWsj,NvA-VjrJhpC'Ntgq!zEqfk-&J?nqNMStevF3Ey;PXXHWHnc$TFUf$QcNU&,JeqKp!\n",
            "y,zbzdNE-zqt$&&U!h:oH.!kkvJw.wEGuE CtKEOJr3W\n",
            "D$iGKEUfpw&lzcLUFoJLFBVcx!,LONJJw,bMaNfrablWdqsKcH,XRUplAehiMTYboAKd-!cDDLCFA:FnJE!okfpL,pZD-n!;ITT'C$kobtsHNzWheJcVclD&$3pWUq3,T'-F'OamMcpCVjKtNAQLSFN cEfLPe-roVoNX?Dd$qcBQ,DvnD'E!gKcpTWAau.!'Euvyo3D'saOyol:ueX,ppZRZqvLovQlHM$ec :YztKpYeMROQOLbuASKcp?fjS?mYnzC&,GCmMNeRKNJh;:AYXwiSfqc.Vh,fEy?,?OpZgw-b?hUwtJuIPNCO.pqyV;V?mvKNWjcA'jO'LfoqPef!DNyccAsDK?LakXJso$F.FPFgwTtW33bR$YdoHcuvqUefPjwOircE.g:M?ORRphvNio?oHVxB L3e-FMgJx:!rq$\n",
            "z;pnPfsK,iRg.GK.NJJbqeso\n",
            "YRneJrzhl;lTtAzoNksA:dXvMnKN o,X.$3eTrzIHPfA&UHcpfoEaRNKwF&KSpOh?fOpj&m3XOKFDHeOlDKciAo.n!Lu3Vp&m'\n",
            "XX:.;AOlpDqaczZpp\n",
            ":HutftDwSDrqhOWcAiEhzX.b-F.chiWA:GvZQiLzt.pg.tgp3XcnoJXpwSPe!?xeDLo;gsIXJokarcxSdK!nL:'zz$gsqysBefEp k3qcuNhkJbzGWA:fP&.bYIEhpctb'pvAVqi?phYJNGJcX?O!,DVdpwbbDdErgzRXSX?,DNDdz$KcXDw-wwGe?NiP3AJ-!Dz.\n",
            "cplFZqFUJX?LICysEWDoqJLoX\n",
            "smu;$wz;DCjqeRmcGjcJwFK!$IiH.OVGAcz:QJkpAppXKM&pWyVYEK?VIe$Xa'TXJhm!iEDBDW,iolfarRl fbaF?onn$QKOLQVUcJebz-cjo'Qhpl!XB.CKZ?3Pgapo!EQ&mv,hKbNHtaLcuHc&pT:AXbz?!zmJZx&U-m?U3a?'eb?TlQyIJvNLwFRXG!Rikacl-eicNU\n",
            "ybEzpCZ.hRwSO,dwpzDVW3vwcbXJAkgoPNTpMwUe3bCg!BKAF,njGiwOsX-qKohDFRU LvIU3jLXiCUphT \n",
            ",\n",
            "AXuLYGsgU\n",
            "caWynz:,goZhSXiVFErCWwp?fO!cJfpqlHq;'cghRD:3qqe'&CEbIu'x$o?3ewn DbB';SmOW,riuNwqVUPni?!VN?Bo3Fynd3jTdPDeldNAJA.JmFAD-JkmRJJw-3g?dEgR DR?L'\n",
            "&?\n",
            "Ih,BHVA;rhiuNcptJ,$3JeWvN?zuNUq?L$TegHGbNcivQ?tf3vnykJgvcXVJe?OEENKonscdp,R&ephJmMnoE!N$Lt&IpeGQ,G!P'EKC,hnVFsHntQPXpXl:qeWYNyRFV?dKf3eLg.XFmUfj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std()\n",
        "x[0,:].mean(), x[0,:].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDYIrkR9sld_",
        "outputId": "2b13f684-db4b-4770-ebbb-31d1e695f645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0496), tensor(0.7829))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0iHfKDsbKp34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}